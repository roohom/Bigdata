# 数据仓库

## 概念

数据库与数据仓库都是事先数据存储的一种设计模型

## 功能

- 更加规范和统一化的数据管理平台
- 将企业需要的所有数据进行存储，提供给企业各个需要使用数据的部门进行应用

## 特点

- 专门用于设计存储数据的地方
- 本身不产生数据
- 本身不使用数据
- 面向主题：不同的应用给定不同的主题，用到不同的数据
  - 对数据按照需求分类

## 流程

- 数据生成
  - 业务数据：数据库
  - 用户行为数据：日志
  - 机器运行日志：日志
  - 爬虫数据
  - 第三方合作数据
- 数据采集：通过各种采集工具将不同类型的数据存储在数据仓库中
  - 先放入HDFS
- ETL：数据清洗
  - 过滤、转换、补全
  - 对HDFS上的原始数据进行处理，处理好的数据放入数仓
- 数据仓库：将各种各样的数据进行统一化的存储
  - 分层：规范所有数据进入数仓以后经过哪些步骤的处理，得到最后想要的结果
    - 原始数据：ETL
    - 第一层：存储ETL之后的数据
    - 第二层：对第一层之后的数据简单处理
    - 第三层：第二层之后的数据进行处理
    - 第四层：得到需求方需要的数据
- 构建数据仓库：
  - 离线数仓：Hive



# Hive

> 起源自：FaceBook

- Hive提供SQL的开发接口，用户可以直接使用SQL来操作Hadoop

- Hive本身只是一个翻译的角色，底层分布式存储和分布式计算都是靠Hadoop来实现的

- 高度依赖于Hadoop

## 本质

一种特殊的支持SQL开发接口的Hadoop客户端



## 功能

- **将文件映射成表的数据**[工作中主要使用此功能构建数仓]
  - Hive的存储：HDFS
- 功能二：将SQL语句转换为MapReduce程序，提交给yarn运行[工作中使用较少，替代品：Impala、SparkSQL
  - MapReduce是对文件进行操作
  - SQL是对表进行操作
  - Hive是对表处理的，底层的MapReduce是对文件进行处理的

## 应用场景

应用于构建**数据仓库**

## 架构

- 客户端：用于提供与用户交互的界面，实现SQL开发

- 服务端：

  - 负责分析SQL，读写元数据，提交程序给Hadoop
  - 连接器：负责维护与客户端的连接
  - 解析器：负责解析SQL语句构建语法树
    - 判断数据库、表是否存在
    - 语法是否正确
    - 最终得到一个逻辑计划
  - 优化器：优化这个逻辑，得到物理计划
  - 执行器：执行物理计划得到结果返回给客户端

- 元数据：存储Hive中关键性信息
  - Hive中所有数据库、所有表的信息
  - HDFS与Hive表的映射关系
  
- Hadoop：Hive所有的请求都是给Hadoop**实现**的
  - Hive自己不是分布式的
  - Hive能实现分布式存储和分布式计算
  - 底层：
    - 存储：HDFS
    
    - 计算：MapReduce+yarn
    
      

## 元数据服务

- 存储内容：**Hive中关键性数据，数据库、表、列的信息**
- 存储位置
  - 默认位置：derby数据库
    - Hive自带的文本型数据库，轻量级的数据库
    - 一般用于嵌入式系统中的数据存储
    - 不方便管理和维护，不方便共享
  - 自定义位置：MySQL 
    - 官方推荐使用的存储方式
    - 工作中使用的方式(**几乎所有的元数据都存放在MySQL**)
- 元数据的**访问**方式：**内嵌模式**、**本地模式**、**远程模式**
  - 内嵌模式：元数据使用默认存储，直接访问derby
  - 本地模式：元数据使用RDBMS(关系型数据库管理系统)：MySQL，Hive
    - RDBMS：关系型数据库管理系统
    - NoSQL：非关系型数据库
  - 远程模式：元数据存储是使用MySQL，Hive服务端访问Metastore服务来访问元数据 (metastroe相当于一个中介)

### 元数据共享

- **问**：使用Spark/Impala/presto等对Hive中的数据进行计算从而代替hive底层的MapReduce计算，如何能让Spark等工具读取到Hive中的表，以及对应的HDFS的数据呢？
  
  - **解决**：所有的数据都存储在元数据中，只要**将Hive元数据共享刚给其他工具即可**
  
  > 默认的，Hive服务端会将Hive客户端的操作请求翻译成MapReduce API并提交给Hadoop，实现对Hive中的数据进行计算，此时Hive仅仅充当一个翻译工具，目的是将用户指定SQL语言翻译成MapReduce代码，而MapReduce运行计算是非常慢的，这样的方式效率低下，而Spark等工具效率高速度快，可以使用spark等工具代替Hive底层的MapReduce实现计算。但是这样一来，spark等工具怎么知道Hive中的表在哪里已经对应的HDFS数据在哪里呢，为了解决此问题，我们知道Hive的元数据中存储了Hive中的关键性信息，如数据库、表、列的信息，只要将Hive中的元数据共享给其他工具即可。
  
- 问：如何实现共享问题？

  - 解决：**构建元数据管理服务MetaStore**，让所有需要访问Jive中表和对应的HDFS数据的工具直接访问MetaStore，MetaStore来告诉他们对应的数据在哪。

  > 如果让spark等其他计算工具直接访问Hive的元数据，会产生一系列权限问题，Hive的元数据是采用MySQL存储的，MySQL会对客户端的访问进行权限检查，使得访问不通过。但即使没有权限检查，其他工具直接访问Hive的元数据，也不清楚不知道访问到的元数据是干嘛的，有怎样的信息，为了解决此问题，Hive专门构建了MetaStore，让其他工具直接将**元数据读写请求**发送至MetaStore，MetaStore会解析客户端的请求，并告诉其需要访问的的数据在哪，以及数据的具体信息。



### 元数据管理服务

- MetaStore：为了实现元数据共享而涉及的一种专有的元数据管理服务

- 元数据管理服务的开启由配置决定，在hive-site.xml中：

  ~~~xml
  <property>
      <name>hive.metastore.uris</name>
      <value>thrift://node3:9083</value>
  </property>
  ~~~

  - 配置了这个服务，就必须先开启MetaStore这个服务再使用Hive

  > 由于所有对Hive元数据的读写请求都是经过MetaStore来处理的，所以必须开启MetaStore服务才能是Hive客户端访问Hive元数据。举个栗子：早期打电话，会有电话中转，张铁妞先将电话拨到服务台，告诉接线员我要打给王大锤，于是接线员就将线路接到了王大锤家，如果MetaStore没有先开启，张铁妞就不能直接拨打王大锤家的电话。

## 表的分类与结构

### 管理表

>  MANAGED_TABLE

- Hive中默认创建的表的类型
  - 特点：
    - 只要不手动删除，这张表就一直存在
    - 手动删除管理表：元数据会被删除，数据也会被删除

### 临时表

> TRMPORARY

- 特点：
  - 这张表创建的客户端一旦断开连接，临时表会自动删除
  - 一般用于存储临时数据，并且表用完以后不会再被使用

### 外部表

> EXTERNAL_TABLE

- 特点：

  - 手动删除外部表：元数据会被删除，但是数据不会被删除

    > 某个用户在读取该表之后将其删除，只是删除了元数据，数据仍然保留在HDFS上，多个人对同一份数据进行读取并建立了外部表，每个人使用完之后删除了自己的表，不影响最终保留在HDFS上的那份数据。

- 应用：

  - 如果这份数据比较重要，建立外部表保证数据安全
  - 入股多个人需要使用这张表读取同一份数据，任何一个表被删除，不能影响数据

### 分区结构表

### 分桶结构表



















